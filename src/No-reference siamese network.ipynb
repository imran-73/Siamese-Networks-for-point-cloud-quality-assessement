{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aac0dfba"},"outputs":[],"source":["import open3d as o3d\n","import tensorflow as tf\n","import numpy as np\n","import os\n","from utilities import *\n","import re\n","import scipy\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import random\n","from pyntcloud import PyntCloud\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from plyfile import PlyData, PlyElement\n","from scipy.spatial import cKDTree\n","from tqdm.notebook import tqdm, trange\n","from multiprocessing import Pool"],"id":"aac0dfba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a4a362b"},"outputs":[],"source":["initial_path = r'write the path where you put the project'\n","noisy_modelnet_pcs_path = os.path.join(initial_path, r'noisy_modelnet_pcs')\n","db_path = os.path.join(initial_path, r'icip2020_perry_quality_repack_bin')\n","deg_metrics_path = os.path.join('data', 'icip2020_deg_metrics.json')\n","degraded_pcs_features_path = os.path.join('data', 'icip2020_degraded_pcs_features.csv')\n","degraded_pcs_features_preds_path = os.path.join('data', 'icip2020_degraded_pcs_features_preds.csv')\n","block_bits = 6\n","block_shape = [2**block_bits] * 3\n","bbox_min = [0,0,0]"],"id":"1a4a362b"},{"cell_type":"markdown","metadata":{"id":"9244bd63"},"source":["## Training on ModelNet dataset"],"id":"9244bd63"},{"cell_type":"code","execution_count":null,"metadata":{"id":"02792be5"},"outputs":[],"source":["train_glob = os.path.join(initial_path, r'ModelNet40_200_pc512_oct3_4k/**/*.ply')\n","files = get_files(train_glob)\n","assert len(files) > 0\n","original_modelnet_pcs = []\n","for path in tqdm(files):\n","    pc = modelnet_pc(path)\n","    pc.load_points(path)\n","    pc.load_tree()\n","    pc.compute_normals()\n","    original_modelnet_pcs.append(pc)"],"id":"02792be5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"03e614d9"},"outputs":[],"source":["std_list=[]\n","for k in range(1):\n","    for i in range(5):\n","        for j in range(5):\n","            std_list.append(i+((0.2)*(j+1)))"],"id":"03e614d9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4f06909"},"outputs":[],"source":["noisy_modelnet_pcs = []\n","d2_list=[]\n","for pc in tqdm(original_modelnet_pcs) :\n","    for std in std_list:\n","        new_pc = make_noisy_version(pc , std)\n","        new_pc.load_tree()\n","        new_pc.load_dists_ngbs()\n","        new_pc.compute_normals()\n","        new_pc.compute_features()\n","        d2 = new_pc.features['min_d2_psnr']\n","        d2_list.append(d2)\n","        noisy_modelnet_pcs.append(pc.content + '__' + pc.pc_name+ '__' + str(std) + '__' + str(d2))\n","        new_pc.write_to_disk(noisy_modelnet_pcs_path, d2)"],"id":"f4f06909"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy7WCQf2g8fe"},"outputs":[],"source":["textfile = open(os.path.join(initial_path,\"info_list_noisy_modelnet_pcs.txt\"), \"w\")\n","for element in noisy_modelnet_pcs :\n","    textfile.write(element + \"\\n\")\n","textfile.close()"],"id":"oy7WCQf2g8fe"},{"cell_type":"code","source":["noisy_modelnet_pcs = []\n","L = open(os.path.join(initial_path,\"info_list_noisy_modelnet_pcs.txt\"), \"r\").read().splitlines();\n","for line in L:\n","  noisy_modelnet_pcs.append(line)"],"metadata":{"id":"pRv5lT8zMRyx"},"id":"pRv5lT8zMRyx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shuffle_and_split(number, train_ratio):\n","    randomlist = np.arange(number)\n","    np.random.shuffle(randomlist)\n","    train_randomlist = randomlist[0:round(number*(1-train_ratio))]\n","    set_randomlist = set(randomlist)\n","    set_train_randomlist = set(train_randomlist) \n","    validation_randomlist = set_randomlist-set_train_randomlist\n","    train_names = np.array(list(set_train_randomlist))\n","    np.random.shuffle(train_names)\n","    validation_names = np.array(list(validation_randomlist))\n","    np.random.shuffle(validation_names)\n","    return train_names, validation_names"],"metadata":{"id":"V-1uKGQdt8-6"},"id":"V-1uKGQdt8-6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def push_example (num): \n","    names_list = noisy_modelnet_pcs\n","    dataset_path = noisy_modelnet_pcs_path\n","    x1_path = names_list[num] \n","    x1 = PyntCloud.from_file(os.path.join(dataset_path, x1_path))\n","    x1_points = x1.points[['x','y','z']].to_numpy()\n","    x1 = o3d.geometry.PointCloud()\n","    x1.points = o3d.utility.Vector3dVector(x1_points)\n","    R1 = x1.get_rotation_matrix_from_xyz((0, 0, np.random.uniform(0, 2) * np.pi))\n","    x1.rotate(R1, center = (32,32,32))\n","    x1.scale(np.random.uniform(0.8,1), center = x1.get_center())\n","    x1 = np.clip(np.asarray(x1.points), 0, 63)#.to_numpy()\n","    zeros1 = np.zeros(block_shape, dtype=np.float32)\n","    x1 = pts_to_vx(x1, block_shape, zeros1)\n","    x1 = x1.reshape([64,64,64,1])\n","    for pc in original_modelnet_pcs: \n","        if names_list[num].split('__')[2] == pc.pc_name:\n","            x2_points = pc.points[['x','y','z']].to_numpy()\n","            x2 = o3d.geometry.PointCloud()\n","            x2.points = o3d.utility.Vector3dVector(x2_points)\n","            R2 = x2.get_rotation_matrix_from_xyz((0, 0, np.random.uniform(0, 2) * np.pi))\n","            x2.rotate(R2, center = (32,32,32))\n","            x2.scale(np.random.uniform(0.8, 1), center = x2.get_center())\n","            x2 = np.clip(np.asarray(x2.points), 0, 63)\n","            break\n","    zeros2 = np.zeros(block_shape, dtype=np.float32)\n","    x2 = pts_to_vx(x2, block_shape, zeros2)\n","    x2 = x2.reshape([64,64,64,1])\n","    d2 = float(names_list[num].split('__')[-1][:-4])\n","    d2 = (d2-75)/75\n","    return x1, x2, d2\n","\n","\n","def tf_push_example (num):\n","    return tf.py_function(push_example,[num], [tf.float32, tf.float32, tf.float64])\n","def divide_example (x1, x2, d2):\n","    return ((x1, x2), d2)"],"metadata":{"id":"Hl1kO7K1uO_j"},"id":"Hl1kO7K1uO_j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_index, val_index = shuffle_and_split(len(noisy_modelnet_pcs), 0.1)\n","train_modelnet_dataset = tf.data.Dataset.from_tensor_slices(train_index)#.take(2)\n","train_modelnet_dataset = train_modelnet_dataset.map(tf_push_example, num_parallel_calls = 64)\n","train_modelnet_dataset = train_modelnet_dataset.map(divide_example, num_parallel_calls = 64)\n","train_modelnet_dataset = train_modelnet_dataset.batch(64)\n","train_modelnet_dataset = train_modelnet_dataset.prefetch(1)\n","val_modelnet_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n","val_modelnet_dataset = val_modelnet_dataset.map(tf_push_example, num_parallel_calls = 64)#.take(2)\n","val_modelnet_dataset = val_modelnet_dataset.map(divide_example, num_parallel_calls = 64)\n","val_modelnet_dataset = val_modelnet_dataset.batch(64)\n","val_modelnet_dataset = val_modelnet_dataset.prefetch(1)"],"metadata":{"id":"KDoJ2qnbuAFf"},"id":"KDoJ2qnbuAFf","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75fab63d"},"outputs":[],"source":["filters=32\n","block_shape_modified=(64,64,64,1)\n","params = {'strides': (2, 2, 2), 'padding': 'same', 'use_bias': True}\n","Embedding = tf.keras.Sequential()\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_0', filters=32, kernel_size=(5, 5, 5), **params, input_shape=block_shape_modified))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_1', filters=32, kernel_size=(5, 5, 5), **params))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_2', filters=32, kernel_size=(5, 5, 5), **params))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_3', filters=8, kernel_size=(1, 1, 1), activation= tf.keras.activations.relu,strides=(1,1,1)))\n","Embedding.add(tf.keras.layers.Flatten(name='flatten'))\n","Embedding.add(tf.keras.layers.Dense(32,activation = 'ReLU', name='dense_0'))\n","Embedding.add(tf.keras.layers.Dense(1,activation = 'ReLU', name='dense_1'))"],"id":"75fab63d"},{"cell_type":"code","source":["right_input = tf.keras.Input((64,64,64,1))\n","left_input = tf.keras.Input((64,64,64,1))\n","right_y = Embedding(right_input)\n","left_y = Embedding(left_input)\n","out = tf.keras.layers.Subtract()([right_y, left_y]) \n","Siamese = tf.keras.Model(inputs = [right_input, left_input ], outputs = [out], name=\"Siamese\")"],"metadata":{"id":"NmzA08bAGnzs"},"id":"NmzA08bAGnzs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_filepath = os.path.join(initial_path, r'chekpoints/siamese_modelnet_gaussnoise')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3)\n","Siamese.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-02), loss=tf.keras.losses.MeanSquaredError()) "],"metadata":{"id":"GVwRb2-RNaUi"},"id":"GVwRb2-RNaUi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=Siamese.fit(train_modelnet_dataset, epochs=100, callbacks=[callback,reduce_lr,checkpoint_callback], initial_epoch=0 ,validation_data=val_modelnet_dataset)"],"metadata":{"id":"jBVFj8U4P0x5"},"id":"jBVFj8U4P0x5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["Embedding_filepath = os.path.join(initial_path, r'chekpoints/Embedding')\n","Embedding.save_weights(Embedding_filepath)"],"metadata":{"id":"HEG6WvGL_hq0"},"id":"HEG6WvGL_hq0","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine tuning and testing on ICIP 2020 dataset"],"metadata":{"id":"mt52td4U-LvN"},"id":"mt52td4U-LvN"},{"cell_type":"code","execution_count":null,"metadata":{"id":"35237644"},"outputs":[],"source":["df = pd.read_csv(os.path.join(db_path, 'dataset.csv'))\n","pc_names = df['pc_name'].unique()\n","df = df.set_index(['pc_name', 'codec_id', 'codec_rate']).sort_index()"],"id":"35237644"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3ffa7fd"},"outputs":[],"source":["icip_pcs = []\n","for idx, data in tqdm(df.iterrows()):\n","    icip_pcs.append(icip_pc(idx[0],idx[1],idx[2],data['geometry_bits'],data['mos'],data['mos_ci'],data['relative_path'],data['radius']))       \n","for pc in tqdm(icip_pcs):\n","    pc.load_points()\n","    pc.connect_with_ref(icip_pcs)\n","    pc.partition()\n","    pc.load_tree()\n","for pc in tqdm(icip_pcs):\n","    pc.load_dists_ngbs()\n","    pc.compute_features()\n","    pc.find_shared_blocks()"],"id":"f3ffa7fd"},{"cell_type":"code","source":["icip_partitions = {}\n","for name in pc_names:\n","    icip_block_names = []\n","    test_names = []\n","    for pc in icip_pcs:\n","        if pc.pc_name != name :\n","            for block in list(pc.blocks_meta.keys()):\n","                icip_block_names.append([pc.id, block])\n","        if pc.pc_name == name :\n","            for block in list(pc.blocks_meta.keys()):\n","                test_names.append([pc.id, block])\n","    icip_partitions[name] = {'train' : icip_block_names, 'test' : test_names}"],"metadata":{"id":"F9kaMLCUAbiD"},"id":"F9kaMLCUAbiD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"073e416f"},"outputs":[],"source":["def shuffle_and_split(number, train_ratio):\n","    randomlist = np.arange(number)\n","    np.random.shuffle(randomlist)\n","    train_randomlist = randomlist[0:round(number*(1-train_ratio))]\n","    set_randomlist = set(randomlist)\n","    set_train_randomlist = set(train_randomlist) \n","    validation_randomlist = set_randomlist-set_train_randomlist\n","    train_names = np.array(list(set_train_randomlist))\n","    np.random.shuffle(train_names)\n","    validation_names = np.array(list(validation_randomlist))\n","    np.random.shuffle(validation_names)\n","    return train_names, validation_names"],"id":"073e416f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ca23c30"},"outputs":[],"source":["def icip_push_sample (num, name, mode): \n","    num = num.numpy()\n","    name = str(name.numpy(),encoding='ascii')\n","    mode = str(mode.numpy(),encoding='ascii')\n","    pc_id = icip_partitions[name][mode][num][0]\n","    block = icip_partitions[name][mode][num][1]\n","    for pc in icip_pcs:\n","        if pc.id == pc_id :\n","            x1 = pc.blocks_meta[block]['block']\n","            mos = pc.mos/5\n","    zeros1 = np.zeros(block_shape, dtype=np.float32)\n","    x1 = pts_to_vx(x1, block_shape, zeros1)\n","    x1 = x1.reshape([64,64,64,1])\n","    return x1, mos\n","def tf_icip_push_sample (num, name, mode):\n","    return tf.py_function(icip_push_sample, [num, name, mode], [tf.float32, tf.float32])\n","def icip_divide_sample (x1, d2):\n","    return (x1, d2)"],"id":"3ca23c30"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b2fa50c0"},"outputs":[],"source":["for pc in icip_pcs: pc.sum_var = 0\n","    \n","for name in tqdm(icip_partitions.keys()):\n","    \n","    icip_block_names = icip_partitions[name]\n","    train_index, val_index = shuffle_and_split(len(icip_block_names), 0.1)\n","    train_dataset = tf.data.Dataset.from_tensor_slices(train_index)#.take(3)\n","    name_dataset = tf.data.Dataset.from_tensor_slices(np.asarray([name for i in range(len(train_index))]))\n","    mode_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(['train' for i in range(len(train_index))]))\n","    icip_dataset = tf.data.Dataset.zip((train_dataset, name_dataset, mode_dataset))\n","    icip_dataset = icip_dataset.map(tf_icip_push_sample, num_parallel_calls = 64)\n","    icip_dataset = icip_dataset.map(icip_divide_sample, num_parallel_calls = 64)\n","    icip_dataset = icip_dataset.batch(64).prefetch(1)\n","    \n","    val_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n","    icip_dataset_val = tf.data.Dataset.zip((val_dataset, name_dataset, mode_dataset))\n","    icip_dataset_val = icip_dataset_val.map(tf_icip_push_sample, num_parallel_calls = 64)\n","    icip_dataset_val = icip_dataset_val.map(icip_divide_sample, num_parallel_calls = 64)\n","    icip_dataset_val = icip_dataset_val.batch(64).prefetch(1)   \n","\n","    Embedding.load_weights(Embedding_filepath)\n","    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n","    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3)\n","    Embedding.trainable = True\n","    Embedding.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-04), loss=tf.keras.losses.MeanSquaredError()) \n","    history = Embedding.fit(icip_dataset, epochs=100, callbacks=[callback,reduce_lr], initial_epoch=0 ,validation_data=icip_dataset_val)\n","    \n","    Embedding.trainable = False\n","    test_names = icip_partitions[name]['test']\n","    name_dataset = tf.data.Dataset.from_tensor_slices(np.asarray([name for i in range(len(test_names))]))\n","    mode_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(['test' for i in range(len(test_names))]))\n","    test_dataset = tf.data.Dataset.from_tensor_slices(np.arange(len(test_names)))#.take(3)\n","    icip_dataset_test = tf.data.Dataset.zip((test_dataset, name_dataset, mode_dataset))\n","    icip_dataset_test = icip_dataset_test.map(tf_icip_push_sample, num_parallel_calls = 128)\n","    icip_dataset_test = icip_dataset_test.map(icip_divide_sample, num_parallel_calls = 128)\n","    icip_dataset_test = icip_dataset_test.batch(128).prefetch(1)\n","    predictions = Embedding.predict(icip_dataset_test)\n","    \n","    for i, elem in enumerate(test_names):\n","        for pc in icip_pcs :\n","            if pc.id == elem[0] :\n","                pc.sum_var = pc.sum_var + predictions[i]"],"id":"b2fa50c0"},{"cell_type":"code","execution_count":null,"metadata":{"id":"90729f50"},"outputs":[],"source":["mos_list = np.reshape(np.asarray([pc.mos for pc in icip_pcs]), -1)\n","preidctions_list = np.reshape(np.asarray([pc.sum_var/pc.num_blocks for pc in icip_pcs]), -1)"],"id":"90729f50"},{"cell_type":"code","execution_count":null,"metadata":{"id":"233b590f"},"outputs":[],"source":["plcc=scipy.stats.pearsonr(mos_list, preidctions_list)\n","srocc=scipy.stats.spearmanr(mos_list, preidctions_list)"],"id":"233b590f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b08228d2"},"outputs":[],"source":["print(plcc)\n","print(srocc)"],"id":"b08228d2"}],"metadata":{"colab":{"collapsed_sections":[],"name":"reseau_2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":5}
