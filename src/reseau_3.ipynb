{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aac0dfba"},"outputs":[],"source":["import open3d as o3d\n","import tensorflow as tf\n","import numpy as np\n","import os\n","from utilities import *\n","import re\n","import scipy\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import random\n","import glob\n","from pyntcloud import PyntCloud\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.linear_model import LinearRegression\n","from plyfile import PlyData, PlyElement\n","from scipy.spatial import cKDTree\n","from tqdm.notebook import tqdm, trange\n","from multiprocessing import Pool"],"id":"aac0dfba"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1a4a362b"},"outputs":[],"source":["initial_path = r'write the path where you put the project'\n","noisy_modelnet_pcs_path = os.path.join(initial_path, r'noisy_modelnet_pcs')\n","db_path = os.path.join(initial_path, r'icip2020_perry_quality_repack_bin')\n","deg_metrics_path = os.path.join('data', 'icip2020_deg_metrics.json')\n","degraded_pcs_features_path = os.path.join('data', 'icip2020_degraded_pcs_features.csv')\n","degraded_pcs_features_preds_path = os.path.join('data', 'icip2020_degraded_pcs_features_preds.csv')\n","block_bits = 6\n","block_shape = [2**block_bits] * 3\n","bbox_min = [0,0,0]"],"id":"1a4a362b"},{"cell_type":"markdown","metadata":{"id":"9244bd63"},"source":["# Training on ModelNet dataset"],"id":"9244bd63"},{"cell_type":"code","execution_count":null,"metadata":{"id":"02792be5"},"outputs":[],"source":["train_glob = os.path.join(initial_path, r'ModelNet40_200_pc512_oct3_4k/**/*.ply')\n","files = get_files(train_glob)\n","assert len(files) > 0\n","original_modelnet_pcs = []\n","for path in tqdm(files):\n","    pc = modelnet_pc(path)\n","    pc.load_points(path)\n","    pc.load_tree()\n","    pc.compute_normals()\n","    original_modelnet_pcs.append(pc)"],"id":"02792be5"},{"cell_type":"code","execution_count":null,"metadata":{"id":"03e614d9"},"outputs":[],"source":["std_list=[]\n","for k in range(1):\n","    for i in range(5):\n","        for j in range(5):\n","            std_list.append(i+((0.2)*(j+1)))"],"id":"03e614d9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4f06909"},"outputs":[],"source":["noisy_modelnet_pcs = []\n","d2_list=[]\n","for pc in tqdm(original_modelnet_pcs) :\n","    for std in std_list:\n","        new_pc = make_noisy_version(pc , std)\n","        new_pc.load_tree()\n","        new_pc.load_dists_ngbs()\n","        new_pc.compute_normals()\n","        new_pc.compute_features()\n","        d2 = new_pc.features['min_d2_psnr']\n","        d2_list.append(d2)\n","        noisy_modelnet_pcs.append(pc.content + '__' + pc.pc_name+ '__' + str(std) + '__' + str(d2))\n","        new_pc.write_to_disk(noisy_modelnet_pcs_path, d2)"],"id":"f4f06909"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy7WCQf2g8fe"},"outputs":[],"source":["textfile = open(os.path.join(initial_path,\"info_list_noisy_modelnet_pcs.txt\"), \"w\")\n","for element in noisy_modelnet_pcs :\n","    textfile.write(element + \"\\n\")\n","textfile.close()"],"id":"oy7WCQf2g8fe"},{"cell_type":"code","source":["noisy_modelnet_pcs = []\n","L = open(os.path.join(initial_path,\"info_list_noisy_modelnet_pcs.txt\"), \"r\").read().splitlines();\n","for line in L:\n","  noisy_modelnet_pcs.append(line)"],"metadata":{"id":"pRv5lT8zMRyx"},"id":"pRv5lT8zMRyx","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def shuffle_and_split(number, train_ratio):\n","    randomlist = np.arange(number)\n","    np.random.shuffle(randomlist)\n","    train_randomlist = randomlist[0:round(number*(1-train_ratio))]\n","    set_randomlist = set(randomlist)\n","    set_train_randomlist = set(train_randomlist) \n","    validation_randomlist = set_randomlist-set_train_randomlist\n","    train_names = np.array(list(set_train_randomlist))\n","    np.random.shuffle(train_names)\n","    validation_names = np.array(list(validation_randomlist))\n","    np.random.shuffle(validation_names)\n","    return train_names, validation_names\n"],"metadata":{"id":"V-1uKGQdt8-6"},"id":"V-1uKGQdt8-6","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def push_example (num): \n","    names_list = noisy_modelnet_pcs\n","    dataset_path = noisy_modelnet_pcs_path\n","    x1_path = names_list[num] \n","    x1 = PyntCloud.from_file(os.path.join(dataset_path, x1_path))\n","    x1_points = x1.points[['x','y','z']].to_numpy()\n","    x1 = o3d.geometry.PointCloud()\n","    x1.points = o3d.utility.Vector3dVector(x1_points)\n","    R1 = x1.get_rotation_matrix_from_xyz((0, 0, np.random.uniform(0, 2) * np.pi))\n","    x1.rotate(R1, center = (32,32,32))\n","    x1.scale(np.random.uniform(0.8,1), center = x1.get_center())\n","    x1 = np.clip(np.asarray(x1.points), 0, 63)#.to_numpy()\n","    zeros1 = np.zeros(block_shape, dtype=np.float32)\n","    x1 = pts_to_vx(x1, block_shape, zeros1)\n","    x1 = x1.reshape([64,64,64,1])\n","    for pc in original_modelnet_pcs: \n","        if names_list[num].split('__')[2] == pc.pc_name:\n","            x2_points = pc.points[['x','y','z']].to_numpy()\n","            x2 = o3d.geometry.PointCloud()\n","            x2.points = o3d.utility.Vector3dVector(x2_points)\n","            R2 = x2.get_rotation_matrix_from_xyz((0, 0, np.random.uniform(0, 2) * np.pi))\n","            x2.rotate(R2, center = (32,32,32))\n","            x2.scale(np.random.uniform(0.8, 1), center = x2.get_center())\n","            x2 = np.clip(np.asarray(x2.points), 0, 63)\n","            break\n","    zeros2 = np.zeros(block_shape, dtype=np.float32)\n","    x2 = pts_to_vx(x2, block_shape, zeros2)\n","    x2 = x2.reshape([64,64,64,1])\n","    d2 = float(names_list[num].split('__')[-1][:-4])/75\n","    return x1, x2, d2\n","\n","def tf_push_example (num):\n","    return tf.py_function(push_example,[num], [tf.float32, tf.float32, tf.float64])\n","def divide_example (x1, x2, d2):\n","    return ((x1, x2), d2)"],"metadata":{"id":"Hl1kO7K1uO_j"},"id":"Hl1kO7K1uO_j","execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_index, val_index = shuffle_and_split(len(noisy_modelnet_pcs), 0.1)\n","train_modelnet_dataset = tf.data.Dataset.from_tensor_slices(train_index)#.take(2)\n","train_modelnet_dataset = train_modelnet_dataset.map(tf_push_example, num_parallel_calls = 64)\n","train_modelnet_dataset = train_modelnet_dataset.map(divide_example, num_parallel_calls = 64)\n","train_modelnet_dataset = train_modelnet_dataset.batch(64)\n","train_modelnet_dataset = train_modelnet_dataset.prefetch(1)\n","val_modelnet_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n","val_modelnet_dataset = val_modelnet_dataset.map(tf_push_example, num_parallel_calls = 64)#.take(2)\n","val_modelnet_dataset = val_modelnet_dataset.map(divide_example, num_parallel_calls = 64)\n","val_modelnet_dataset = val_modelnet_dataset.batch(64)\n","val_modelnet_dataset = val_modelnet_dataset.prefetch(1)"],"metadata":{"id":"KDoJ2qnbuAFf"},"id":"KDoJ2qnbuAFf","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75fab63d"},"outputs":[],"source":["filters=32\n","block_shape_modified=(64,64,64,1)\n","params = {'strides': (2, 2, 2), 'padding': 'same', 'use_bias': True}\n","Embedding = tf.keras.Sequential()\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_0', filters=32, kernel_size=(5, 5, 5), **params, input_shape=block_shape_modified))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_1', filters=32, kernel_size=(5, 5, 5), **params))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_2', filters=32, kernel_size=(5, 5, 5), **params))\n","Embedding.add(tf.keras.layers.ReLU())\n","Embedding.add(tf.keras.layers.Conv3D(name='conv3d_3', filters=8, kernel_size=(1, 1, 1), activation= tf.keras.activations.relu,strides=(1,1,1)))\n","Embedding.add(tf.keras.layers.Flatten(name='flatten'))"],"id":"75fab63d"},{"cell_type":"code","source":["right_input = tf.keras.Input((64,64,64,1))\n","left_input = tf.keras.Input((64,64,64,1))\n","right_y = Embedding(right_input)\n","left_y = Embedding(left_input)\n","b = tf.keras.layers.Concatenate()([right_y, left_y]) \n","b=tf.keras.layers.Dense(32,activation='relu')(b)\n","b=tf.keras.layers.Dense(4,activation='relu')(b)\n","b=tf.keras.layers.Dense(1,activation='relu')(b)\n","Siamese = tf.keras.Model(inputs = [right_input, left_input ], outputs = [b], name=\"siamese\")"],"metadata":{"id":"NmzA08bAGnzs"},"id":"NmzA08bAGnzs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_filepath = os.path.join(initial_path, r'chekpoints/experience_2_siamese')\n","checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","        checkpoint_filepath,\n","        monitor=\"val_loss\",\n","        save_best_only=True,\n","        save_weights_only=True,\n","    )\n","callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n","reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3)\n","Siamese.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-02), loss=tf.keras.losses.MeanSquaredError()) "],"metadata":{"id":"GVwRb2-RNaUi"},"id":"GVwRb2-RNaUi","execution_count":null,"outputs":[]},{"cell_type":"code","source":["history=Siamese.fit(train_modelnet_dataset, epochs=100, callbacks=[callback,reduce_lr,checkpoint_callback], initial_epoch=0 ,validation_data=val_modelnet_dataset)"],"metadata":{"id":"jBVFj8U4P0x5"},"id":"jBVFj8U4P0x5","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training the Pointnet network (taken from Keras website)"],"metadata":{"id":"0HbtPJDjSdyb"},"id":"0HbtPJDjSdyb"},{"cell_type":"code","source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","import glob\n","import trimesh\n","NUM_POINTS = 2048\n","NUM_CLASSES = 10\n","BATCH_SIZE = 32"],"metadata":{"id":"N5v3CCBnZOGD"},"id":"N5v3CCBnZOGD","execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATA_DIR = tf.keras.utils.get_file(\n","    \"modelnet.zip\",\n","    \"http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\",\n","    extract=True,\n",")\n","DATA_DIR = os.path.join(os.path.dirname(DATA_DIR), \"ModelNet10\")\n","\n","def parse_dataset(num_points=2048):\n","\n","    train_points = []\n","    train_labels = []\n","    test_points = []\n","    test_labels = []\n","    class_map = {}\n","    folders = glob.glob(os.path.join(DATA_DIR, \"[!README]*\"))\n","\n","    for i, folder in enumerate(folders):\n","        print(\"processing class: {}\".format(os.path.basename(folder)))\n","        # store folder name with ID so we can retrieve later\n","        class_map[i] = folder.split(\"/\")[-1]\n","        # gather all files\n","        train_files = glob.glob(os.path.join(folder, \"train/*\"))\n","        test_files = glob.glob(os.path.join(folder, \"test/*\"))\n","\n","        for f in train_files:\n","            train_points.append(trimesh.load(f).sample(num_points))\n","            train_labels.append(i)\n","\n","        for f in test_files:\n","            test_points.append(trimesh.load(f).sample(num_points))\n","            test_labels.append(i)\n","\n","    return (\n","        np.array(train_points),\n","        np.array(test_points),\n","        np.array(train_labels),\n","        np.array(test_labels),\n","        class_map,\n","    )\n","    \n","train_points, test_points, train_labels, test_labels, CLASS_MAP = parse_dataset(\n","    NUM_POINTS\n",")\n","\n","def augment(points, label):\n","    # jitter points\n","    points += tf.random.uniform(points.shape, -0.005, 0.005, dtype=tf.float64)\n","    # shuffle points\n","    points = tf.random.shuffle(points)\n","    return points, label\n","\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices((train_points, train_labels))\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_points, test_labels))\n","\n","train_dataset = train_dataset.shuffle(len(train_points)).map(augment).batch(BATCH_SIZE)\n","test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n"],"metadata":{"id":"GY9u0pfqkFi0"},"id":"GY9u0pfqkFi0","execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","### Build a model\n","Each convolution and fully-connected layer (with exception for end layers) consits of\n","Convolution / Dense -> Batch Normalization -> ReLU Activation.\n","\"\"\"\n","\n","def conv_bn(x, filters):\n","    x = layers.Conv1D(filters, kernel_size=1, padding=\"valid\")(x)\n","    x = layers.BatchNormalization(momentum=0.0)(x)\n","    return layers.Activation(\"relu\")(x)\n","\n","\n","def dense_bn(x, filters):\n","    x = layers.Dense(filters)(x)\n","    x = layers.BatchNormalization(momentum=0.0)(x)\n","    return layers.Activation(\"relu\")(x)\n","\n","\n","\"\"\"\n","PointNet consists of two core components. The primary MLP network, and the transformer\n","net (T-net). The T-net aims to learn an affine transformation matrix by its own mini\n","network. The T-net is used twice. The first time to transform the input features (n, 3)\n","into a canonical representation. The second is an affine transformation for alignment in\n","feature space (n, 3). As per the original paper we constrain the transformation to be\n","close to an orthogonal matrix (i.e. ||X*X^T - I|| = 0).\n","\"\"\"\n","\n","\n","class OrthogonalRegularizer(keras.regularizers.Regularizer):\n","    def __init__(self, num_features, l2reg=0.001):\n","        self.num_features = num_features\n","        self.l2reg = l2reg\n","        self.eye = tf.eye(num_features)\n","\n","    def __call__(self, x):\n","        x = tf.reshape(x, (-1, self.num_features, self.num_features))\n","        xxt = tf.tensordot(x, x, axes=(2, 2))\n","        xxt = tf.reshape(xxt, (-1, self.num_features, self.num_features))\n","        return tf.reduce_sum(self.l2reg * tf.square(xxt - self.eye))\n","\n","\n","\"\"\"\n"," We can then define a general function to build T-net layers.\n","\"\"\"\n","\n","\n","def tnet(inputs, num_features):\n","\n","    # Initalise bias as the indentity matrix\n","    bias = keras.initializers.Constant(np.eye(num_features).flatten())\n","    reg = OrthogonalRegularizer(num_features)\n","\n","    x = conv_bn(inputs, 32)\n","    x = conv_bn(x, 64)\n","    x = conv_bn(x, 512)\n","    x = layers.GlobalMaxPooling1D()(x)\n","    x = dense_bn(x, 256)\n","    x = dense_bn(x, 128)\n","    x = layers.Dense(\n","        num_features * num_features,\n","        kernel_initializer=\"zeros\",\n","        bias_initializer=bias,\n","        activity_regularizer=reg,\n","    )(x)\n","    feat_T = layers.Reshape((num_features, num_features))(x)\n","    # Apply affine transformation to input features\n","    return layers.Dot(axes=(2, 1))([inputs, feat_T])\n","\n","\n","\"\"\"\n","The main network can be then implemented in the same manner where the t-net mini models\n","can be dropped in a layers in the graph. Here we replicate the network architecture\n","published in the original paper but with half the number of weights at each layer as we\n","are using the smaller 10 class ModelNet dataset.\n","\"\"\"\n","\n","inputs = keras.Input(shape=(NUM_POINTS, 3))\n","\n","x = tnet(inputs, 3)\n","x = conv_bn(x, 32)\n","x = conv_bn(x, 32)\n","x = tnet(x, 32)\n","x = conv_bn(x, 32)\n","x = conv_bn(x, 64)\n","x = conv_bn(x, 512)\n","x = layers.GlobalMaxPooling1D()(x)\n","x = dense_bn(x, 256)\n","x = layers.Dropout(0.3)(x)\n","x = dense_bn(x, 128)\n","x = layers.Dropout(0.3)(x)\n","\n","outputs = layers.Dense(NUM_CLASSES, activation=\"softmax\")(x)\n","\n","pointnet = keras.Model(inputs=inputs, outputs=outputs, name=\"pointnet\")\n"],"metadata":{"id":"NL6VLCtP8DGm"},"id":"NL6VLCtP8DGm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pointnet.compile(\n","    loss=\"sparse_categorical_crossentropy\",\n","    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n","    metrics=[\"sparse_categorical_accuracy\"],\n",")\n","\n","pointnet.fit(train_dataset, epochs=20, validation_data=test_dataset)"],"metadata":{"id":"yr8b7SJOZ-1O"},"id":"yr8b7SJOZ-1O","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Fine tuning and testing on ICIP 2020 dataset"],"metadata":{"id":"UsyaEefZS19-"},"id":"UsyaEefZS19-"},{"cell_type":"code","source":["def build_model(num_inputs, num_dense):\n","    layers = []\n","    inputs = []\n","    p_input=tf.keras.Input(shape=(2048, 3),name='point_input')\n","    for i in range(num_inputs):\n","        right_input = tf.keras.Input((64,64,64,1))\n","        left_input = tf.keras.Input((64,64,64,1))\n","        siamese =  Siamese([right_input, left_input])\n","        layers.append(siamese)\n","        inputs.append(right_input)\n","        inputs.append(left_input)\n","    inputs.append(p_input)\n","    layers.append(pointnet(p_input))\n","    context_layer = tf.keras.layers.Concatenate()(layers) \n","    dense1 = tf.keras.layers.Dense(num_dense, name='Dense1',activation='relu')(context_layer)\n","    dense2 = tf.keras.layers.Dense(1, name='Output', activation='sigmoid')(dense1)\n","    net = tf.keras.Model(inputs,dense2)\n","    net.compile(optimizer=tf.keras.optimizers.Adam(),loss=tf.keras.losses.MeanSquaredError()) \n","    return net"],"metadata":{"id":"N_4F-qWp848F"},"id":"N_4F-qWp848F","execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_inputs = 20\n","num_dense = 16\n","net = build_model(num_inputs, num_dense)"],"metadata":{"id":"MyhjZdxr9Qgf"},"id":"MyhjZdxr9Qgf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["net_checkpoint_filepath = os.path.join(initial_path, r'chekpoints/net')\n","net.save_weights(net_checkpoint_filepath)"],"metadata":{"id":"fM4LFPMHHDKg"},"id":"fM4LFPMHHDKg","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"35237644"},"outputs":[],"source":["df = pd.read_csv(os.path.join(db_path, 'dataset.csv'))\n","pc_names = df['pc_name'].unique()\n","df = df.set_index(['pc_name', 'codec_id', 'codec_rate']).sort_index()"],"id":"35237644"},{"cell_type":"code","execution_count":null,"metadata":{"id":"f3ffa7fd"},"outputs":[],"source":["icip_pcs = []\n","for idx, data in tqdm(df.iterrows()):\n","    icip_pcs.append(icip_pc(idx[0],idx[1],idx[2],data['geometry_bits'],data['mos'],data['mos_ci'],data['relative_path'],data['radius']))       \n","for pc in tqdm(icip_pcs):\n","    pc.load_points()\n","    pc.connect_with_ref(icip_pcs)\n","    pc.partition()\n","    pc.load_tree()\n","for pc in tqdm(icip_pcs):\n","    pc.load_dists_ngbs()\n","    pc.compute_features()\n","    pc.find_shared_blocks()\n","    pc.downsample(downsampled_size=2048)"],"id":"f3ffa7fd"},{"cell_type":"code","source":["icip_partitions = {}\n","for name in pc_names:\n","    test_dic = {}\n","    train_dic = {}\n","\n","    ids_train_set = list(set([pc.id for pc in icip_pcs if (pc.pc_name != name and pc.is_ref == False)]))\n","    for id in ids_train_set :\n","        id_blocks = []\n","        for pc in icip_pcs:\n","          if pc.id == id :\n","            for block in pc.shared_blocks:\n","                id_blocks.append([block])\n","        train_dic[id] = id_blocks\n","\n","    ids_test_set = list(set([pc.id for pc in icip_pcs if (pc.pc_name == name and pc.is_ref == False)]))\n","    for id in ids_test_set :\n","        id_blocks = []\n","        for pc in icip_pcs:\n","          if pc.id == id :\n","            for block in pc.shared_blocks:\n","                id_blocks.append([block])\n","        test_dic[id] = id_blocks\n","\n","    icip_partitions[name] = {'train' : train_dic, 'test' : test_dic}"],"metadata":{"id":"F9kaMLCUAbiD"},"id":"F9kaMLCUAbiD","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ca23c30"},"outputs":[],"source":["def icip_push_sample (num):\n","    num = num.numpy()\n","    pc_id = ids[num]\n","    blocks = train_list[num]\n","    inputs = []\n","    for pc in icip_pcs:\n","      if pc.id == pc_id :\n","        for block in blocks:\n","          x1 = pc.blocks_meta[block]['block']\n","          zeros1 = np.zeros(block_shape, dtype=np.float32)\n","          x1 = pts_to_vx(x1, block_shape, zeros1)\n","          x1 = x1.reshape([64,64,64,1])\n","          x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n","          x2 = pc.ref.blocks_meta[block]['block']\n","          zeros2 = np.zeros(block_shape, dtype=np.float32)\n","          x2 = pts_to_vx(x2, block_shape, zeros2)\n","          x2 = x2.reshape([64,64,64,1])\n","          x2 = tf.convert_to_tensor(x2, dtype=tf.float32)\n","          inputs.append(x1)\n","          inputs.append(x2)\n","        break\n","    for pc in icip_pcs:\n","        if pc.id == pc_id :\n","            inputs.append(pc.downsampled)\n","            break\n","    inputs = tuple(inputs)\n","    return inputs\n","def tf_icip_push_sample (num):\n","    return tf.py_function(icip_push_sample, [num], ((num_inputs*2)+1) * [tf.float32])\n","def icip_divide_sample (x1, d2):\n","    return (x1, d2)\n","def push_mos(num):\n","    num = num.numpy()\n","    pc_id = ids[num]\n","    blocks = train_list[num]\n","    for pc in icip_pcs:\n","        if pc.id == pc_id :\n","            mos = pc.mos/5\n","            break\n","    return mos\n","def tf_push_mos (num):\n","    return tf.py_function(push_mos, [num], [tf.float32])\n","\n","def test_icip_push_sample (num):\n","    num = num.numpy()\n","    pc_id = ids[num]\n","    blocks = test_list[num]\n","    inputs = []\n","    for pc in icip_pcs:\n","      if pc.id == pc_id :\n","        for block in blocks:\n","          x1 = pc.blocks_meta[block]['block']\n","          zeros1 = np.zeros(block_shape, dtype=np.float32)\n","          x1 = pts_to_vx(x1, block_shape, zeros1)\n","          x1 = x1.reshape([1,64,64,64,1])\n","          x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n","          x2 = pc.ref.blocks_meta[block]['block']\n","          zeros2 = np.zeros(block_shape, dtype=np.float32)\n","          x2 = pts_to_vx(x2, block_shape, zeros2)\n","          x2 = x2.reshape([1,64,64,64,1])\n","          x2 = tf.convert_to_tensor(x2, dtype=tf.float32)\n","          inputs.append(x1)\n","          inputs.append(x2)\n","        break\n","    for pc in icip_pcs:\n","        if pc.id == pc_id :\n","            inputs.append(pc.downsampled.reshape([1,2048,3]))\n","            break\n","    inputs = tuple(inputs)\n","    return inputs\n","def tf_test_icip_push_sample (num):\n","    return tf.py_function(test_icip_push_sample, [num], ((num_inputs*2)+1) * [tf.float32])"],"id":"3ca23c30"},{"cell_type":"code","source":["for name in tqdm(icip_partitions.keys()):\n","    train_list = []\n","    ids = list(icip_partitions[name]['train'].keys())\n","    for id in ids :\n","      chosen_idxs = np.random.choice(range(len(list(icip_partitions[name]['train'][id]))), num_inputs, replace = False)\n","      chosen_blocks = [list(icip_partitions[name]['train'][id])[x][0] for x in chosen_idxs]\n","      train_list.append(chosen_blocks)\n","\n","    pre_dataset = tf.data.Dataset.range(75)\n","\n","    icip_dataset = pre_dataset.map(tf_icip_push_sample, num_parallel_calls = 75)\n","    mos_dataset =  pre_dataset.map(tf_push_mos)\n","    dataset = tf.data.Dataset.zip((icip_dataset, mos_dataset))\n","    dataset = dataset.shuffle(75)\n","    train_dataset = dataset.take(70)\n","    train_dataset =train_dataset.batch(1).prefetch(1)\n","    val_dataset = dataset.skip(70)\n","    val_dataset = dataset.batch(1).prefetch(1)\n","    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n","    net.load_weights(net_checkpoint_filepath)\n","    net.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-04), loss=tf.keras.losses.MeanSquaredError())\n","    net.fit(train_dataset, epochs=100, callbacks=[callback,reduce_lr], initial_epoch=0 ,validation_data=val_dataset)\n","    \n","    test_list = []\n","    ids = list(icip_partitions[name]['test'].keys())\n","    for id in ids :\n","      chosen_idxs = np.random.choice(range(len(list(icip_partitions[name]['test'][id]))), num_inputs, replace = False)\n","      chosen_blocks = [list(icip_partitions[name]['test'][id])[x][0] for x in chosen_idxs]\n","      test_list.append(chosen_blocks)\n","\n","    pre_dataset = tf.data.Dataset.range(len(ids))\n","    test_dataset = pre_dataset.map(tf_test_icip_push_sample, num_parallel_calls = len(ids))\n","    predictions = []\n","    for i, elem in enumerate(list(test_dataset.as_numpy_iterator())) :\n","      id = ids[i]\n","      for pc in icip_pcs:\n","        if pc.id == id:\n","          pc.predicted_mos = net(elem)\n","          break"],"metadata":{"id":"3K1G8AVbdIla"},"id":"3K1G8AVbdIla","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90729f50"},"outputs":[],"source":["mos_list = np.reshape(np.asarray([pc.mos for pc in icip_pcs]), -1)\n","preidctions_list = np.reshape(np.asarray([float(pc.predicted_mos)*5 for pc in icip_pcs]), -1)"],"id":"90729f50"},{"cell_type":"code","execution_count":null,"metadata":{"id":"233b590f"},"outputs":[],"source":["plcc=scipy.stats.pearsonr(mos_list, preidctions_list)\n","srocc=scipy.stats.spearmanr(mos_list, preidctions_list)"],"id":"233b590f"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b08228d2"},"outputs":[],"source":["print(plcc)\n","print(srocc)"],"id":"b08228d2"}],"metadata":{"colab":{"collapsed_sections":[],"name":"reseau_3.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}