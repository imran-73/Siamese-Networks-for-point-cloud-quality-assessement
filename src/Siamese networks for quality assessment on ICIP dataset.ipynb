{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac0dfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from utilities import *\n",
    "import re\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from pyntcloud import PyntCloud\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from plyfile import PlyData, PlyElement\n",
    "from scipy.spatial import cKDTree\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from multiprocessing import Pool\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "#mpu.default_rc_params(rcParams);\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a4a362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path = r'../icip2020_perry_quality_repack_bin'\n",
    "waterloo_path = r'../src/the_WPC_database'\n",
    "deg_metrics_path = os.path.join('data', 'icip2020_deg_metrics.json')\n",
    "degraded_pcs_features_path = os.path.join('data', 'icip2020_degraded_pcs_features.csv')\n",
    "degraded_pcs_features_preds_path = os.path.join('data', 'icip2020_degraded_pcs_features_preds.csv')\n",
    "block_bits = 6\n",
    "block_shape = [2**block_bits] * 3\n",
    "bbox_min = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35237644",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(db_path, 'dataset.csv'))\n",
    "pc_names = df['pc_name'].unique()\n",
    "df = df.set_index(['pc_name', 'codec_id', 'codec_rate']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffa7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_pcs = []\n",
    "for idx, data in tqdm(df.iterrows()):\n",
    "    icip_pcs.append(icip_pc(idx[0],idx[1],idx[2],data['geometry_bits'],data['mos'],data['mos_ci'],data['relative_path'],data['radius']))       \n",
    "for pc in tqdm(icip_pcs):\n",
    "    pc.load_points()\n",
    "    pc.connect_with_ref(icip_pcs)\n",
    "    pc.partition()\n",
    "#    pc.load_tree()\n",
    "for pc in tqdm(icip_pcs):\n",
    "#    pc.load_dists_ngbs()\n",
    "#    pc.compute_features()\n",
    "    pc.find_shared_blocks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0521c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "icip_partitions = {}\n",
    "for name in pc_names:\n",
    "    icip_block_names = []\n",
    "    test_names = []\n",
    "    for pc in icip_pcs:\n",
    "        if pc.pc_name != name and pc.is_ref == False :\n",
    "            for block in pc.shared_blocks:\n",
    "                icip_block_names.append([pc.id, block])\n",
    "        if pc.pc_name == name and pc.is_ref == False:\n",
    "            for block in pc.shared_blocks:\n",
    "                test_names.append([pc.id, block])\n",
    "    icip_partitions[name] = {'train' : icip_block_names, 'test' : test_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75fab63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filters=32\n",
    "block_shape_modified=(64,64,64,1)\n",
    "params = {'strides': (2, 2, 2), 'padding': 'same', 'use_bias': True}\n",
    "Embedding = tf.keras.Sequential()\n",
    "Embedding.add(tf.keras.layers.Conv3D(name='conv3d_0', filters=32, kernel_size=(5, 5, 5), **params, input_shape=block_shape_modified))\n",
    "Embedding.add(tf.keras.layers.ReLU())\n",
    "Embedding.add(tf.keras.layers.Conv3D(name='conv3d_1', filters=32, kernel_size=(5, 5, 5), **params))\n",
    "Embedding.add(tf.keras.layers.ReLU())\n",
    "Embedding.add(tf.keras.layers.Conv3D(name='conv3d_2', filters=32, kernel_size=(5, 5, 5), **params))\n",
    "Embedding.add(tf.keras.layers.ReLU())\n",
    "Embedding.add(tf.keras.layers.Conv3D(name='conv3d_3', filters=8, kernel_size=(1, 1, 1), activation= tf.keras.activations.relu,strides=(1,1,1)))\n",
    "Embedding.add(tf.keras.layers.Flatten(name='flatten'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be84a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "right_input = tf.keras.Input((64,64,64,1))\n",
    "left_input = tf.keras.Input((64,64,64,1))\n",
    "right_y = Embedding(right_input)\n",
    "left_y = Embedding(left_input)\n",
    "b = tf.keras.layers.Concatenate()([right_y, left_y]) \n",
    "b = tf.keras.layers.Dropout(rate=0.5)(b)\n",
    "b=tf.keras.layers.Dense(32,activation='relu')(b)\n",
    "b = tf.keras.layers.Dropout(rate=0.5)(b)\n",
    "b=tf.keras.layers.Dense(4,activation='relu')(b)\n",
    "b=tf.keras.layers.Dense(1,activation='relu')(b)\n",
    "Siamese = tf.keras.Model(inputs = [right_input, left_input ], outputs = [b], name=\"siamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "073e416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_split(number, train_ratio):\n",
    "    randomlist = np.arange(number)\n",
    "    np.random.shuffle(randomlist)\n",
    "    train_randomlist = randomlist[0:round(number*(1-train_ratio))]\n",
    "    set_randomlist = set(randomlist)\n",
    "    set_train_randomlist = set(train_randomlist) \n",
    "    validation_randomlist = set_randomlist-set_train_randomlist\n",
    "    train_names = np.array(list(set_train_randomlist))\n",
    "    np.random.shuffle(train_names)\n",
    "    validation_names = np.array(list(validation_randomlist))\n",
    "    np.random.shuffle(validation_names)\n",
    "    return train_names, validation_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ca23c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icip_push_sample (num, name, mode): \n",
    "    #print(type(num))\n",
    "    num = num.numpy()\n",
    "    name = str(name.numpy(),encoding='ascii')\n",
    "    mode = str(mode.numpy(),encoding='ascii')\n",
    "    pc_id = icip_partitions[name][mode][num][0]\n",
    "    block = icip_partitions[name][mode][num][1]\n",
    "    #block = icip_block_names[num][1]\n",
    "    for pc in icip_pcs:\n",
    "        if pc.id == pc_id :\n",
    "            x1 = pc.blocks_meta[block]['block']\n",
    "            x2 = pc.ref.blocks_meta[block]['block']\n",
    "            mos = pc.mos/5\n",
    "    zeros1 = np.zeros(block_shape, dtype=np.float32)\n",
    "    x1 = pts_to_vx(x1, block_shape, zeros1)\n",
    "    x1 = x1.reshape([64,64,64,1])\n",
    "    zeros2 = np.zeros(block_shape, dtype=np.float32)\n",
    "    x2 = pts_to_vx(x2, block_shape, zeros2)\n",
    "    x2 = x2.reshape([64,64,64,1])\n",
    "    return x1, x2, mos\n",
    "def tf_icip_push_sample (num, name, mode):\n",
    "    return tf.py_function(icip_push_sample, [num, name, mode], [tf.float32, tf.float32, tf.float32])\n",
    "def icip_divide_sample (x1, x2, d2):\n",
    "    return ((x1, x2), d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce8439e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model, weights=None):\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    initializer = tf.keras.initializers.HeUniform()\n",
    "    weights = [initializer(shape = w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5143cbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 1e-03\n",
    "def lr_exp_decay(epoch, lr):\n",
    "    k = 0.1\n",
    "    return initial_learning_rate * tf.math.exp(-k*epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fa50c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc in icip_pcs: pc.sum_var = 0\n",
    "    \n",
    "for name in tqdm(icip_partitions.keys()):\n",
    "    \n",
    "    icip_block_names = icip_partitions[name]['train']\n",
    "    train_index, val_index = shuffle_and_split(len(icip_block_names), 0.1)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(train_index)#.take(3)\n",
    "    name_dataset = tf.data.Dataset.from_tensor_slices(np.asarray([name for i in range(len(train_index))]))\n",
    "    mode_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(['train' for i in range(len(train_index))]))\n",
    "    icip_dataset = tf.data.Dataset.zip((train_dataset, name_dataset, mode_dataset))\n",
    "    icip_dataset = icip_dataset.map(tf_icip_push_sample, num_parallel_calls = 64)\n",
    "    icip_dataset = icip_dataset.map(icip_divide_sample, num_parallel_calls = 64)\n",
    "    icip_dataset = icip_dataset.batch(64).prefetch(1)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(val_index)\n",
    "    icip_dataset_val = tf.data.Dataset.zip((val_dataset, name_dataset, mode_dataset))\n",
    "    icip_dataset_val = icip_dataset_val.map(tf_icip_push_sample, num_parallel_calls = 64)\n",
    "    icip_dataset_val = icip_dataset_val.map(icip_divide_sample, num_parallel_calls = 64)\n",
    "    icip_dataset_val = icip_dataset_val.batch(64).prefetch(1)   \n",
    "    #siamese_checkpoint_filepath = '../new_chkpts/siamese_modelnet_10e6_gaussnoise'\n",
    "    #Siamese.load_weights(siamese_checkpoint_filepath)\n",
    "    initialize_weights(Siamese)\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)\n",
    "    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3)\n",
    "    lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=0)\n",
    "    Siamese.trainable = True\n",
    "    Siamese.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-03), loss=tf.keras.losses.MeanSquaredError()) \n",
    "    history = Siamese.fit(icip_dataset, epochs=100, callbacks=[callback,reduce_lr,lr_scheduler], initial_epoch=0 ,validation_data=icip_dataset_val)\n",
    "    \n",
    "    Siamese.trainable = False\n",
    "    Siamese.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.MeanSquaredError())\n",
    "    test_names = icip_partitions[name]['test']\n",
    "    mode_dataset = tf.data.Dataset.from_tensor_slices(np.asarray(['test' for i in range(len(test_names))]))\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices(np.arange(len(test_names)))#.take(3)\n",
    "    icip_dataset_test = tf.data.Dataset.zip((test_dataset, name_dataset, mode_dataset))\n",
    "    icip_dataset_test = icip_dataset_test.map(tf_icip_push_sample, num_parallel_calls = 128)\n",
    "    icip_dataset_test = icip_dataset_test.map(icip_divide_sample, num_parallel_calls = 128)\n",
    "    icip_dataset_test = icip_dataset_test.batch(128).prefetch(1)\n",
    "    predictions = Siamese.predict(icip_dataset_test)\n",
    "    icip_partitions[name]['predictions'] = predictions\n",
    "    \n",
    "    for i, elem in enumerate(test_names):\n",
    "        for pc in icip_pcs :\n",
    "            if pc.id == elem[0] :\n",
    "                pc.sum_var = pc.sum_var + predictions[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90729f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "mos_list = np.reshape(np.asarray([pc.mos for pc in icip_pcs if pc.is_ref == False][0:75]), -1)\n",
    "preidctions_list = np.reshape(np.asarray([pc.sum_var/len(pc.shared_blocks) for pc in icip_pcs if pc.is_ref == False][0:75]), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "233b590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plcc=scipy.stats.pearsonr(mos_list, preidctions_list)\n",
    "srocc=scipy.stats.spearmanr(mos_list, preidctions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b08228d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8356561950371263, 1.1160330465589438e-20)\n",
      "SpearmanrResult(correlation=0.8348971058303803, pvalue=1.3018166846360089e-20)\n"
     ]
    }
   ],
   "source": [
    "print(plcc)\n",
    "print(srocc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfpy38",
   "language": "python",
   "name": "tfpy38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
